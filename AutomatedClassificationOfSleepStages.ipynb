{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CnnModel.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "OZd8wO8jDxiD",
        "BoZPXYhpEAQM"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVaQ_rNP1RRh"
      },
      "source": [
        "## IMPORTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3imGHiwcT_F3",
        "outputId": "757c84cb-4255-4310-c143-ab8b9e66d340"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZXcEv9LarlU"
      },
      "source": [
        "from torch.utils.data import Dataset\r\n",
        "import torch\r\n",
        "from torch.nn import Conv2d, BatchNorm2d, ReLU, Linear, Dropout, ModuleList, LogSoftmax, LayerNorm\r\n",
        "import datetime\r\n",
        "import os\r\n",
        "import pandas as pd\r\n",
        "from scipy.signal import butter, filtfilt, resample\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "from sklearn.metrics import balanced_accuracy_score\r\n",
        "from sklearn.utils import compute_class_weight\r\n",
        "from torch.nn import NLLLoss\r\n",
        "from torch.optim import RMSprop\r\n",
        "from torch.optim.lr_scheduler import LambdaLR\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mwpiWkcUTpAa",
        "outputId": "698c00db-4de8-42cd-d742-3d11afad50e6"
      },
      "source": [
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Tesla T4'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZd8wO8jDxiD"
      },
      "source": [
        "## PREPROCESSING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKLaRxaSFQ41"
      },
      "source": [
        "### CONSTANTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9fp8QrpatAc"
      },
      "source": [
        "# define the constants influencing the preprocessing part\r\n",
        "CUTOFF = 25.6\r\n",
        "\r\n",
        "SAMPLING_RATE = 128\r\n",
        "\r\n",
        "DESIRED_DOWNSAMPLE_RATE = 64\r\n",
        "\r\n",
        "SUBJECT_EPOCHS_TRAIN = 21600\r\n",
        "\r\n",
        "SUBJECT_EPOCHS_TEST = 21600\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YK6wo-SAD6aN"
      },
      "source": [
        "### PREPROCESSING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2tnvHhO1a_-"
      },
      "source": [
        "### Read and parse the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfNbkTstavkh"
      },
      "source": [
        "# define data paths\r\n",
        "base_path = os.path.join('drive','MyDrive','task3_data')\r\n",
        "x_train_path_eeg1 = os.path.join(base_path, 'train_eeg1.csv')\r\n",
        "x_train_path_eeg2 = os.path.join(base_path, 'train_eeg2.csv')\r\n",
        "x_train_path_emg = os.path.join(base_path, 'train_emg.csv')\r\n",
        "x_test_path_emg = os.path.join(base_path, 'test_emg.csv')\r\n",
        "x_test_path_eeg1 = os.path.join(base_path, 'test_eeg1.csv')\r\n",
        "x_test_path_eeg2 = os.path.join(base_path, 'test_eeg2.csv')\r\n",
        "y_path = os.path.join(base_path, 'train_labels.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wal9zJSzazMf"
      },
      "source": [
        "# read the data\r\n",
        "df_X_train_eeg1 = pd.read_csv(x_train_path_eeg1, float_precision = \"round_trip\", memory_map = True, engine = 'c')\r\n",
        "df_X_train_eeg2 = pd.read_csv(x_train_path_eeg2, float_precision = \"round_trip\", memory_map = True, engine = 'c')\r\n",
        "df_X_train_emg = pd.read_csv(x_train_path_emg, float_precision = \"round_trip\", memory_map = True, engine = 'c')\r\n",
        "df_X_test_eeg1 = pd.read_csv(x_test_path_eeg1, float_precision = \"round_trip\", memory_map = True, engine = 'c')\r\n",
        "df_X_test_eeg2 = pd.read_csv(x_test_path_eeg2, float_precision = \"round_trip\", memory_map = True, engine = 'c')\r\n",
        "df_X_test_emg = pd.read_csv(x_test_path_emg, float_precision = \"round_trip\", memory_map = True, engine = 'c')\r\n",
        "df_y = pd.read_csv(y_path, float_precision = \"round_trip\", memory_map = True, engine = 'c')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsX3HP6h9g5g"
      },
      "source": [
        "# get the arrays\r\n",
        "X_train_eeg1 = df_X_train_eeg1.to_numpy()[:, 1:]\r\n",
        "X_train_eeg2 = df_X_train_eeg2.to_numpy()[:, 1:]\r\n",
        "X_train_emg = df_X_train_emg.to_numpy()[:, 1:]\r\n",
        "X_test_eeg1 = df_X_test_eeg1.to_numpy()[:, 1:]\r\n",
        "X_test_eeg2 = df_X_test_eeg2.to_numpy()[:, 1:]\r\n",
        "X_test_emg = df_X_test_emg.to_numpy()[:, 1:]\r\n",
        "y = df_y.to_numpy()[:, 1:]\r\n",
        "\r\n",
        "# delete panda arrays\r\n",
        "df_X_train_eeg1 = None\r\n",
        "df_X_train_eeg2 = None\r\n",
        "df_X_train_emg = None\r\n",
        "df_X_test_eeg1 = None\r\n",
        "df_X_test_eeg2 = None\r\n",
        "df_X_test_emg = None\r\n",
        "df_y = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjivIRd81kI8"
      },
      "source": [
        "### Apply filtering and Downsampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WCFdj8b9qmw"
      },
      "source": [
        "def butter_bandpass(low, fs, order = 4, type = \"lowpass\"):\r\n",
        "    nyq = 0.5 * fs\r\n",
        "    low = low / nyq\r\n",
        "    b, a = butter(order, low, btype = type, analog = True, output = \"ba\")\r\n",
        "    return b, a\r\n",
        "\r\n",
        "\r\n",
        "def filter_array(data):\r\n",
        "    filter_b, filter_a = butter_bandpass(CUTOFF, SAMPLING_RATE)\r\n",
        "    for index in range(len(data)):\r\n",
        "        data[index] = filtfilt(filter_b, filter_a, data[index])\r\n",
        "\r\n",
        "    return data\r\n",
        "\r\n",
        "\r\n",
        "def downsample_signal(data):\r\n",
        "    seconds = len(data[0]) // SAMPLING_RATE\r\n",
        "    new_sampling_rate = seconds * DESIRED_DOWNSAMPLE_RATE\r\n",
        "\r\n",
        "    new_data = []\r\n",
        "    for index in range(len(data)):\r\n",
        "        new_data.append(resample(data[index], new_sampling_rate))\r\n",
        "\r\n",
        "    return np.array(new_data)\r\n",
        "\r\n",
        "\r\n",
        "def preprocess_data(X_train_eeg1, X_train_eeg2, X_train_emg, X_test_eeg1, X_test_eeg2, X_test_emg):\r\n",
        "    # apply filtering\r\n",
        "    X_train_eeg1 = filter_array(X_train_eeg1)\r\n",
        "    X_train_eeg2 = filter_array(X_train_eeg2)\r\n",
        "    X_train_emg = filter_array(X_train_emg)\r\n",
        "    X_test_eeg1 = filter_array(X_test_eeg1)\r\n",
        "    X_test_eeg2 = filter_array(X_test_eeg2)\r\n",
        "    X_test_emg = filter_array(X_test_emg)\r\n",
        "\r\n",
        "    # downsample signal\r\n",
        "    X_train_eeg1 = downsample_signal(X_train_eeg1)\r\n",
        "    X_train_eeg2 = downsample_signal(X_train_eeg2)\r\n",
        "    X_train_emg = downsample_signal(X_train_emg)\r\n",
        "    X_test_eeg1 = downsample_signal(X_test_eeg1)\r\n",
        "    X_test_eeg2 = downsample_signal(X_test_eeg2)\r\n",
        "    X_test_emg = downsample_signal(X_test_emg)\r\n",
        "\r\n",
        "    return X_train_eeg1, X_train_eeg2, X_train_emg, X_test_eeg1, X_test_eeg2, X_test_emg\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLTC8nxo-I30"
      },
      "source": [
        " # apply filtering\r\n",
        "X_train_eeg1 = filter_array(X_train_eeg1)\r\n",
        "X_train_eeg2 = filter_array(X_train_eeg2)\r\n",
        "X_train_emg = filter_array(X_train_emg)\r\n",
        "X_test_eeg1 = filter_array(X_test_eeg1)\r\n",
        "X_test_eeg2 = filter_array(X_test_eeg2)\r\n",
        "X_test_emg = filter_array(X_test_emg)\r\n",
        "\r\n",
        "# downsample signal\r\n",
        "X_train_eeg1 = downsample_signal(X_train_eeg1)\r\n",
        "X_train_eeg2 = downsample_signal(X_train_eeg2)\r\n",
        "X_train_emg = downsample_signal(X_train_emg)\r\n",
        "X_test_eeg1 = downsample_signal(X_test_eeg1)\r\n",
        "X_test_eeg2 = downsample_signal(X_test_eeg2)\r\n",
        "X_test_emg = downsample_signal(X_test_emg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Uj9ftxm1taf"
      },
      "source": [
        "### Reshape data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rE21Ktbw-gxs"
      },
      "source": [
        "# define limits for each subject\r\n",
        "first_subject_start_train = 2\r\n",
        "first_subject_end_train = SUBJECT_EPOCHS_TRAIN - 3\r\n",
        "second_subject_start_train = SUBJECT_EPOCHS_TRAIN + 2\r\n",
        "second_subject_end_train = 2 * SUBJECT_EPOCHS_TRAIN - 3\r\n",
        "third_subject_start_train = 2 * SUBJECT_EPOCHS_TRAIN + 2\r\n",
        "third_subject_end_train = 3 * SUBJECT_EPOCHS_TRAIN - 3\r\n",
        "fourth_subject_start_test = 2\r\n",
        "fourth_subject_end_test = SUBJECT_EPOCHS_TEST - 3\r\n",
        "fifth_subject_start_test = SUBJECT_EPOCHS_TEST + 2\r\n",
        "fifth_subject_end_test = 2 * SUBJECT_EPOCHS_TEST - 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2i8d06G0-wWH"
      },
      "source": [
        "def get_5_epochs(data, index):\r\n",
        "    return np.concatenate((\r\n",
        "        data[index - 2],\r\n",
        "        data[index - 1],\r\n",
        "        data[index],\r\n",
        "        data[index + 1],\r\n",
        "        data[index + 2],\r\n",
        "    ))\r\n",
        "\r\n",
        "def get_subject_data(start, end, eeg1, eeg2, emg, y):\r\n",
        "    new_eeg_1 = []\r\n",
        "    new_eeg_2 = []\r\n",
        "    new_emg = []\r\n",
        "    new_y = []\r\n",
        "\r\n",
        "    for index in range(start, end + 1):\r\n",
        "      new_eeg_1.append(\r\n",
        "          get_5_epochs(eeg1, index)\r\n",
        "      )\r\n",
        "      new_eeg_2.append(\r\n",
        "          get_5_epochs(eeg2, index)\r\n",
        "      )\r\n",
        "      new_emg.append(\r\n",
        "          get_5_epochs(emg, index)\r\n",
        "      )\r\n",
        "      if y is not None:\r\n",
        "        new_y.append(y[index])\r\n",
        "\r\n",
        "    return np.array(new_eeg_1),np.array(new_eeg_2),np.array(new_emg), np.array(new_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hgkzv3XT_sS_"
      },
      "source": [
        "# get first subject\r\n",
        "first_eeg_1, first_eeg_2, first_emg, first_y = get_subject_data(first_subject_start_train, \r\n",
        "                                                                first_subject_end_train, \r\n",
        "                                                                X_train_eeg1,\r\n",
        "                                                                X_train_eeg2,\r\n",
        "                                                                X_train_emg, \r\n",
        "                                                                y)\r\n",
        "\r\n",
        "# get second subject\r\n",
        "second_eeg_1, second_eeg_2, second_emg, second_y = get_subject_data(second_subject_start_train, \r\n",
        "                                                                second_subject_end_train, \r\n",
        "                                                                X_train_eeg1,\r\n",
        "                                                                X_train_eeg2,\r\n",
        "                                                                X_train_emg, \r\n",
        "                                                                y)\r\n",
        "\r\n",
        "# get third subject\r\n",
        "third_eeg_1, third_eeg_2, third_emg, third_y = get_subject_data(third_subject_start_train, \r\n",
        "                                                                third_subject_end_train, \r\n",
        "                                                                X_train_eeg1,\r\n",
        "                                                                X_train_eeg2,\r\n",
        "                                                                X_train_emg, \r\n",
        "                                                                y)\r\n",
        "\r\n",
        "# get fourth subject\r\n",
        "fourth_eeg_1, fourth_eeg_2, fourth_emg, _ = get_subject_data(fourth_subject_start_test, \r\n",
        "                                                                fourth_subject_end_test, \r\n",
        "                                                                X_test_eeg1,\r\n",
        "                                                                X_test_eeg2,\r\n",
        "                                                                X_test_emg, \r\n",
        "                                                                None)\r\n",
        "\r\n",
        "# get fifth subject\r\n",
        "fifth_eeg_1, fifth_eeg_2, fifth_emg, _ = get_subject_data(fifth_subject_start_test, \r\n",
        "                                                                fifth_subject_end_test, \r\n",
        "                                                                X_test_eeg1,\r\n",
        "                                                                X_test_eeg2,\r\n",
        "                                                                X_test_emg, \r\n",
        "                                                                None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARClpAV7CFJk"
      },
      "source": [
        "## LEARNING\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoZPXYhpEAQM"
      },
      "source": [
        "### DATASET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5p0QJYNUCBdw"
      },
      "source": [
        "class CnnDataset(Dataset):\r\n",
        "    def __init__(self, data, y):\r\n",
        "        self.data = data\r\n",
        "        self.y = y\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.data)\r\n",
        "\r\n",
        "    def __getitem__(self, item):\r\n",
        "        return np.array([self.data[item]]), self.y[item], item\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_F0UoeJkEDfn"
      },
      "source": [
        "### MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6uUHCT8EDNg"
      },
      "source": [
        "\r\n",
        "class CNNModel(torch.nn.Module):\r\n",
        "    def __init__(self, number_of_channels, cnn_blocks, example_length, kernel_size, number_of_signals,\r\n",
        "                 nn_first_layer_size, nn_second_layer_size, dropout):\r\n",
        "        super(CNNModel, self).__init__()\r\n",
        "\r\n",
        "        cnn_layers = []\r\n",
        "\r\n",
        "        # add a BatchNorm before the first layer\r\n",
        "        cnn_layers.append(\r\n",
        "            BatchNorm2d(1, momentum = 0.99, track_running_stats = False)\r\n",
        "            # LayerNorm(example_length)\r\n",
        "        )\r\n",
        "\r\n",
        "        # add 1x1 convolution to increase number of channels\r\n",
        "        cnn_layers.append(\r\n",
        "            Conv2d(\r\n",
        "                in_channels = 1,\r\n",
        "                out_channels = 64,\r\n",
        "                kernel_size = [1, 1],\r\n",
        "                stride = 1\r\n",
        "            )\r\n",
        "        )\r\n",
        "\r\n",
        "        # add the CNN blocks\r\n",
        "        for index in range(cnn_blocks):\r\n",
        "            # add the CNN block\r\n",
        "            cnn_layers.append(\r\n",
        "                Conv2d(\r\n",
        "                    in_channels = 64,\r\n",
        "                    out_channels = 64,\r\n",
        "                    kernel_size = [1, kernel_size],\r\n",
        "                    stride = [1, index % 2 + 1]  # even => 1; odd => 2\r\n",
        "                )\r\n",
        "            )\r\n",
        "\r\n",
        "            # add the Relu Layer\r\n",
        "            cnn_layers.append(\r\n",
        "                ReLU()\r\n",
        "            )\r\n",
        "\r\n",
        "            # update example length\r\n",
        "            example_length = (example_length - kernel_size) // (index % 2 + 1) + 1\r\n",
        "\r\n",
        "            # add the BatchNorm layer\r\n",
        "            cnn_layers.append(\r\n",
        "                BatchNorm2d(number_of_channels, momentum = 0.99, track_running_stats = False)\r\n",
        "                # LayerNorm(example_length)\r\n",
        "            )\r\n",
        "\r\n",
        "            \r\n",
        "\r\n",
        "        # define nn layers\r\n",
        "        nn_layers = []\r\n",
        "\r\n",
        "        # get the size per batch after flatten and concatenate\r\n",
        "        nn_size = example_length * number_of_channels * number_of_signals\r\n",
        "\r\n",
        "        # add dropout\r\n",
        "        nn_layers.append(\r\n",
        "            Dropout(\r\n",
        "                p = dropout\r\n",
        "            )\r\n",
        "        )\r\n",
        "\r\n",
        "        # define the NN first layer\r\n",
        "        nn_layers.append(\r\n",
        "            Linear(\r\n",
        "                nn_size,\r\n",
        "                nn_first_layer_size\r\n",
        "            )\r\n",
        "        )\r\n",
        "\r\n",
        "        # add dropout\r\n",
        "        nn_layers.append(\r\n",
        "            Dropout(\r\n",
        "                p = dropout\r\n",
        "            )\r\n",
        "        )\r\n",
        "\r\n",
        "        # add ReLU\r\n",
        "        nn_layers.append(\r\n",
        "            ReLU()\r\n",
        "        )\r\n",
        "\r\n",
        "        # add the NN second layer\r\n",
        "        nn_layers.append(\r\n",
        "            Linear(\r\n",
        "                nn_first_layer_size,\r\n",
        "                nn_second_layer_size\r\n",
        "            )\r\n",
        "        )\r\n",
        "\r\n",
        "        # add the Softmax\r\n",
        "        nn_layers.append(\r\n",
        "            LogSoftmax(dim = 1)\r\n",
        "        )\r\n",
        "\r\n",
        "        # add layers as object field\r\n",
        "        self.nn_layers = ModuleList(nn_layers)\r\n",
        "        self.cnn_layers = ModuleList(cnn_layers)\r\n",
        "\r\n",
        "    def forward(self, tensor):\r\n",
        "\r\n",
        "        # pass through each cnn layer\r\n",
        "        for index, layer in enumerate(self.cnn_layers):\r\n",
        "            tensor = layer(tensor)\r\n",
        "\r\n",
        "        # flatten along all dimensions except batch\r\n",
        "        tensor = tensor.view(tensor.size(0), 1, 1, tensor.size(1) * tensor.size(2) * tensor.size(3))\r\n",
        "\r\n",
        "        # eliminate irrelevant dimensions\r\n",
        "        tensor = tensor[:, 0, 0, :]\r\n",
        "\r\n",
        "        # pass through each nn layer\r\n",
        "        for index, layer in enumerate(self.nn_layers):\r\n",
        "            tensor = layer(tensor)\r\n",
        "\r\n",
        "        return tensor\r\n",
        "\r\n",
        "    def save_model_to_file(self, full_path):\r\n",
        "        \"\"\"\r\n",
        "            Saves the current model to a file in order to be able to use it later\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        # save model\r\n",
        "        torch.save(self.state_dict(), full_path)\r\n",
        "\r\n",
        "    def load_model_from_file(self, full_path):\r\n",
        "        \"\"\"\r\n",
        "            Load model from file\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        # load model\r\n",
        "        self.load_state_dict(torch.load(full_path))\r\n",
        "\r\n",
        "        # necessary step for loading\r\n",
        "        self.eval()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWatUbIBpNxQ"
      },
      "source": [
        "### Weight Init"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTIi1RKapMxr"
      },
      "source": [
        "# ==================================== WEIGHT INITIALIZER ===========================================\r\n",
        "class WeightInitializer:\r\n",
        "    '''\r\n",
        "    Utiility class for initializing the weights of a network.\r\n",
        "\r\n",
        "    Usage example:\r\n",
        "        weightInit = WeightInitializer()\r\n",
        "        weightInit.init_weights(model, 'xavier_normal_', {'gain':0.02})\r\n",
        "\r\n",
        "    '''\r\n",
        "\r\n",
        "    def __init__(self, initType = None, kwargs = { }):\r\n",
        "        self.kwargs = kwargs\r\n",
        "        self.weightInit = None\r\n",
        "\r\n",
        "        if initType is not None:\r\n",
        "            if not hasattr(torch.nn, initType):\r\n",
        "                raise NotImplementedError('Init method [%s] does not exist in torch.nn' % initType)\r\n",
        "            self.weightInit = getattr(torch.nn.init, initType)\r\n",
        "\r\n",
        "    # ===============================================  INIT WEIGHTS =================================\r\n",
        "    def init_weights(self, model, weightInit = None, kwargs = { }):\r\n",
        "        '''\r\n",
        "        Function called for initializeing the weights of a model\r\n",
        "        :param model: pytorch model\r\n",
        "        :param weightInit: init type (must be in torch.nn.init.*)\r\n",
        "        :param kwargs: kwargs to be passed to the initialization function\r\n",
        "        :return:\r\n",
        "        '''\r\n",
        "\r\n",
        "        if weightInit is not None:\r\n",
        "            if not hasattr(torch.nn.init, weightInit):\r\n",
        "                raise NotImplementedError('Init method %s not in torch.nn' % weightInit)\r\n",
        "            self.weightInit = getattr(torch.nn.init, weightInit)\r\n",
        "\r\n",
        "        self.kwargs = kwargs if kwargs != { } else self.kwargs\r\n",
        "\r\n",
        "        model.apply(self._init_module)\r\n",
        "\r\n",
        "    # =============================================== INIT MODULES ====================================================\r\n",
        "    def _init_module(self, module):\r\n",
        "        '''\r\n",
        "        Internal function which is applied to every module in a network\r\n",
        "\r\n",
        "        :param module: model to be applied to\r\n",
        "        '''\r\n",
        "\r\n",
        "        className = module.__class__.__name__\r\n",
        "\r\n",
        "        # init conv and linear layers\r\n",
        "        if hasattr(module, 'weight') and (className.find('Conv') != -1 or className.find('Linear') != -1):\r\n",
        "            self.weightInit(module.weight.data, **self.kwargs)\r\n",
        "            # init biases\r\n",
        "            if hasattr(module, 'bias') and module.bias is not None:\r\n",
        "                torch.nn.init.constant_(module.bias.data, 0.0)\r\n",
        "\r\n",
        "        # init batch norm weightd\r\n",
        "        elif className.find(\r\n",
        "                'BatchNorm2d') != -1:  # BatchNorm Layer's weight is not a matrix; only normal distribution applies.\r\n",
        "            torch.nn.init.normal_(module.weight.data, 1.0, 0.02)\r\n",
        "            torch.nn.init.constant_(module.bias.data, 0.0)\r\n",
        "\r\n",
        "    # =============================================== INIT NET =====================================================\r\n",
        "    def parallel_net(self, net, gpus = []):\r\n",
        "\r\n",
        "        assert (torch.cuda.is_available()), 'Cuda is not available'\r\n",
        "        assert len(gpus) > 0, 'GPU id not specified'\r\n",
        "        net = torch.nn.DataParallel(net, gpus)  # multi-GPUs\r\n",
        "\r\n",
        "        return net\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWyCg6MfEN1q"
      },
      "source": [
        "### TRAIN ENGINE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpabaK4qEQCE"
      },
      "source": [
        "class TrainEngine():\r\n",
        "    def __init__(self, data_dict, batch_size, smoothing_factor, max_norm, learning_rate_delta, warm_up_epochs,\r\n",
        "                 normal_epochs, cooldown_epochs, number_of_channels, cnn_blocks, example_length, kernel_size,\r\n",
        "                 number_of_signals, nn_first_layer_size, nn_second_layer_size, dropout, use_gpu):\r\n",
        "        # save parameters\r\n",
        "        self.data_dict = data_dict\r\n",
        "        self.batch_size = batch_size\r\n",
        "        self.smoothing_factor = smoothing_factor\r\n",
        "        self.max_norm = max_norm\r\n",
        "        self.learning_rate_delta = learning_rate_delta\r\n",
        "        self.warm_up_epochs = warm_up_epochs\r\n",
        "        self.normal_epochs = normal_epochs\r\n",
        "        self.cooldown_epochs = cooldown_epochs\r\n",
        "        self.number_of_channels = number_of_channels\r\n",
        "        self.cnn_blocks = cnn_blocks\r\n",
        "        self.example_length = example_length\r\n",
        "        self.kernel_size = kernel_size\r\n",
        "        self.number_of_signals = number_of_signals\r\n",
        "        self.nn_first_layer_size = nn_first_layer_size\r\n",
        "        self.nn_second_layer_size = nn_second_layer_size\r\n",
        "        self.dropout = dropout\r\n",
        "\r\n",
        "        # move model to gpu or cpu\r\n",
        "        if use_gpu:\r\n",
        "            self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "        else:\r\n",
        "            self.device = torch.device(\"cpu\")\r\n",
        "\r\n",
        "        # define learning rate\r\n",
        "        self.learning_rate = self.learning_rate_delta * batch_size\r\n",
        "        self.learning_rate_step_warm = self.learning_rate / (self.warm_up_epochs + 1)\r\n",
        "        self.learning_rate_step_cold = self.learning_rate / (self.cooldown_epochs + 1)\r\n",
        "\r\n",
        "    def initialize_engine(self):\r\n",
        "\r\n",
        "        # define model\r\n",
        "        self.model = CNNModel(\r\n",
        "            number_of_channels = self.number_of_channels,\r\n",
        "            cnn_blocks = self.cnn_blocks,\r\n",
        "            example_length = self.example_length,\r\n",
        "            kernel_size = self.kernel_size,\r\n",
        "            number_of_signals = self.number_of_signals,\r\n",
        "            nn_first_layer_size = self.nn_first_layer_size,\r\n",
        "            nn_second_layer_size = self.nn_second_layer_size,\r\n",
        "            dropout = self.dropout\r\n",
        "        )\r\n",
        "        self.model.to(self.device)\r\n",
        "        self.model = self.model.float()\r\n",
        "\r\n",
        "        # initialize weights\r\n",
        "        weightInit = WeightInitializer()\r\n",
        "        weightInit.init_weights(self.model, 'xavier_normal_', { 'gain': 0.02 })\r\n",
        "\r\n",
        "        # define optimizer\r\n",
        "        self.optimizer = RMSprop(self.model.parameters(), lr = self.learning_rate_step_warm,\r\n",
        "                                 alpha = self.smoothing_factor)\r\n",
        "\r\n",
        "        # define learning rate algorithm\r\n",
        "        self.scheduler = LambdaLR(self.optimizer, lr_lambda = lambda epoch: self.get_learning_rate(epoch))\r\n",
        "\r\n",
        "    def get_labels(self, prediction):\r\n",
        "        labels = torch.argmax(prediction, dim = 1)\r\n",
        "        return labels + 1\r\n",
        "\r\n",
        "    def train(self, path):\r\n",
        "        \"\"\"\r\n",
        "        Training with cross validation using the LOOCV principle on each subject.\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        for cross in ['first', 'second', 'third']:\r\n",
        "            # for cross in ['third']:\r\n",
        "\r\n",
        "            # define loaders\r\n",
        "            train = ['first', 'second', 'third']\r\n",
        "            train.remove(cross)\r\n",
        "            train_loader, cross_loader, class_weights = self.create_loaders(train, cross)\r\n",
        "\r\n",
        "            # define loss\r\n",
        "            self.loss = NLLLoss(weight = torch.FloatTensor(class_weights).to(self.device))\r\n",
        "\r\n",
        "            # define engine\r\n",
        "            self.initialize_engine()\r\n",
        "\r\n",
        "            train_loss = []\r\n",
        "            for epoch in range(self.warm_up_epochs + self.normal_epochs + self.cooldown_epochs):\r\n",
        "\r\n",
        "                # set model on train mode\r\n",
        "                self.model.train()\r\n",
        "\r\n",
        "                print(f'Epoch: {epoch + 1}. Start time: {str(datetime.datetime.now())}')\r\n",
        "\r\n",
        "                real_array = np.array([])\r\n",
        "                predicted_array = np.array([])\r\n",
        "\r\n",
        "                count = 0\r\n",
        "\r\n",
        "                # for each batch\r\n",
        "                for batch, real, _ in train_loader:\r\n",
        "                    count += 1\r\n",
        "\r\n",
        "                    # move to device\r\n",
        "                    batch = batch.to(self.device)\r\n",
        "                    real = real.to(self.device)\r\n",
        "\r\n",
        "                    # change to float type\r\n",
        "                    batch = batch.float()\r\n",
        "                    real = real.long()\r\n",
        "\r\n",
        "                    # set optimizer to 0\r\n",
        "                    self.optimizer.zero_grad()\r\n",
        "\r\n",
        "                    # get model prediction\r\n",
        "                    prediction = self.model(batch)\r\n",
        "\r\n",
        "                    # get labels\r\n",
        "                    predicted_labels = self.get_labels(prediction)\r\n",
        "\r\n",
        "                    # compute loss\r\n",
        "                    loss = self.loss(prediction, real - 1)\r\n",
        "\r\n",
        "                    # compute gradient\r\n",
        "                    loss.backward()\r\n",
        "\r\n",
        "                    # clip the gradient\r\n",
        "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_norm)\r\n",
        "\r\n",
        "                    # update model\r\n",
        "                    self.optimizer.step()\r\n",
        "\r\n",
        "                    predicted_labels = predicted_labels.cpu().detach().numpy()\r\n",
        "                    real = real.cpu().detach().numpy()\r\n",
        "                    predicted_array = np.concatenate((predicted_array, predicted_labels))\r\n",
        "                    real_array = np.concatenate((real_array, real))\r\n",
        "\r\n",
        "                train_loss.append(balanced_accuracy_score(real_array, predicted_array))\r\n",
        "                print(f'Train loss: {train_loss[-1]}')\r\n",
        "\r\n",
        "                # update learning rate\r\n",
        "                self.scheduler.step()\r\n",
        "\r\n",
        "                # move model to eval mode\r\n",
        "                self.model.eval()\r\n",
        "\r\n",
        "                real_array = np.array([])\r\n",
        "                predicted_array = np.array([])\r\n",
        "                item_array = np.array([])\r\n",
        "                predicted_probabilities = None\r\n",
        "\r\n",
        "                count = 0\r\n",
        "\r\n",
        "                # for each batch\r\n",
        "                for batch, real, item in cross_loader:\r\n",
        "                    count += 1\r\n",
        "\r\n",
        "                    # move to device\r\n",
        "                    batch = batch.to(self.device)\r\n",
        "                    real = real.to(self.device)\r\n",
        "\r\n",
        "                    # change to float type\r\n",
        "                    batch = batch.float()\r\n",
        "                    real = real.long()\r\n",
        "\r\n",
        "                    # get model prediction\r\n",
        "                    prediction = self.model(batch)\r\n",
        "\r\n",
        "                    # get labels\r\n",
        "                    predicted_labels = self.get_labels(prediction)\r\n",
        "\r\n",
        "                    predicted_labels = predicted_labels.cpu().detach().numpy()\r\n",
        "                    real = real.cpu().detach().numpy()\r\n",
        "                    item = item.cpu().detach().numpy()\r\n",
        "                    predicted_array = np.concatenate((predicted_array, predicted_labels))\r\n",
        "                    real_array = np.concatenate((real_array, real))\r\n",
        "                    item_array = np.concatenate((item_array, item))\r\n",
        "\r\n",
        "                    if predicted_probabilities is None:\r\n",
        "                        predicted_probabilities = prediction.cpu().detach().numpy()\r\n",
        "                    else:\r\n",
        "                        predicted_probabilities = np.concatenate(\r\n",
        "                            (predicted_probabilities, prediction.cpu().detach().numpy()))\r\n",
        "\r\n",
        "                print(f'Cross loss: {balanced_accuracy_score(real_array, predicted_array)}')\r\n",
        "\r\n",
        "            # save predicted labels\r\n",
        "            predicted_rearranged = [0 for _ in range(len(item_array))]\r\n",
        "            for index, item in enumerate(item_array):\r\n",
        "                item = int(item)\r\n",
        "                predicted_rearranged[item] = predicted_probabilities[index]\r\n",
        "            np.save(os.path.join(path, f'{cross}_prediction.npy'), predicted_rearranged)\r\n",
        "\r\n",
        "        return real_array, predicted_array\r\n",
        "\r\n",
        "    def full_train(self, path):\r\n",
        "        \"\"\"\r\n",
        "        Full training of the model, using all the available data.\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        # define the training and test set\r\n",
        "        train_data = np.concatenate(\r\n",
        "            (self.data_dict['first']['train'], self.data_dict['second']['train'], self.data_dict['third']['train']))\r\n",
        "        train_y = np.concatenate(\r\n",
        "            (self.data_dict['first']['y'], self.data_dict['second']['y'], self.data_dict['third']['y']))\r\n",
        "\r\n",
        "        test_data = np.concatenate(\r\n",
        "            (self.data_dict['fourth']['test'], self.data_dict['fifth']['test'])\r\n",
        "        )\r\n",
        "        test_y = np.array([0 for _ in range(test_data.shape[0])])\r\n",
        "\r\n",
        "        dataset_train = CnnDataset(train_data, train_y)\r\n",
        "        train_loader = DataLoader(dataset_train, batch_size = self.batch_size, shuffle = True)\r\n",
        "\r\n",
        "        dataset_test = CnnDataset(test_data, test_y)\r\n",
        "        test_loader = DataLoader(dataset_test, batch_size = self.batch_size, shuffle = True)\r\n",
        "\r\n",
        "        class_weights = compute_class_weight('balanced', np.unique(train_y), np.array(train_y))\r\n",
        "\r\n",
        "        # define loss\r\n",
        "        self.loss = NLLLoss(weight = torch.FloatTensor(class_weights).to(self.device))\r\n",
        "\r\n",
        "        # define engine\r\n",
        "        self.initialize_engine()\r\n",
        "\r\n",
        "        train_loss = []\r\n",
        "        for epoch in range(self.warm_up_epochs + self.normal_epochs + self.cooldown_epochs):\r\n",
        "\r\n",
        "            # set model on train mode\r\n",
        "            self.model.train()\r\n",
        "\r\n",
        "            print(f'Epoch: {epoch + 1}. Start time: {str(datetime.datetime.now())}')\r\n",
        "\r\n",
        "            real_array = np.array([])\r\n",
        "            predicted_array = np.array([])\r\n",
        "\r\n",
        "            count = 0\r\n",
        "\r\n",
        "            # for each batch\r\n",
        "            for batch, real, _ in train_loader:\r\n",
        "                count += 1\r\n",
        "\r\n",
        "                # move to device\r\n",
        "                batch = batch.to(self.device)\r\n",
        "                real = real.to(self.device)\r\n",
        "\r\n",
        "                # change to float type\r\n",
        "                batch = batch.float()\r\n",
        "                real = real.long()\r\n",
        "\r\n",
        "                # set optimizer to 0\r\n",
        "                self.optimizer.zero_grad()\r\n",
        "\r\n",
        "                # get model prediction\r\n",
        "                prediction = self.model(batch)\r\n",
        "\r\n",
        "                # get labels\r\n",
        "                predicted_labels = self.get_labels(prediction)\r\n",
        "\r\n",
        "                # compute loss\r\n",
        "                loss = self.loss(prediction, real - 1)\r\n",
        "\r\n",
        "                # compute gradient\r\n",
        "                loss.backward()\r\n",
        "\r\n",
        "                # clip the gradient\r\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_norm)\r\n",
        "\r\n",
        "                # update model\r\n",
        "                self.optimizer.step()\r\n",
        "\r\n",
        "                predicted_labels = predicted_labels.cpu().detach().numpy()\r\n",
        "                real = real.cpu().detach().numpy()\r\n",
        "                predicted_array = np.concatenate((predicted_array, predicted_labels))\r\n",
        "                real_array = np.concatenate((real_array, real))\r\n",
        "\r\n",
        "            train_loss.append(balanced_accuracy_score(real_array, predicted_array))\r\n",
        "            print(f'Train loss: {train_loss[-1]}')\r\n",
        "\r\n",
        "            # update learning rate\r\n",
        "            self.scheduler.step()\r\n",
        "\r\n",
        "        # move model to eval mode\r\n",
        "        self.model.eval()\r\n",
        "\r\n",
        "        predicted_array = np.array([])\r\n",
        "        predicted_probabilities = None\r\n",
        "        item_array = np.array([])\r\n",
        "\r\n",
        "        count = 0\r\n",
        "\r\n",
        "        # for each batch\r\n",
        "        for batch, _, item in test_loader:\r\n",
        "            count += 1\r\n",
        "\r\n",
        "            # move to device\r\n",
        "            batch = batch.to(self.device)\r\n",
        "\r\n",
        "            # change to float type\r\n",
        "            batch = batch.float()\r\n",
        "\r\n",
        "            # get model prediction\r\n",
        "            prediction = self.model(batch)\r\n",
        "\r\n",
        "            # get labels\r\n",
        "            predicted_labels = self.get_labels(prediction)\r\n",
        "\r\n",
        "            predicted_labels = predicted_labels.cpu().detach().numpy()\r\n",
        "            item = item.cpu().detach().numpy()\r\n",
        "            item_array = np.concatenate((item_array, item))\r\n",
        "            predicted_array = np.concatenate((predicted_array, predicted_labels))\r\n",
        "            if predicted_probabilities is None:\r\n",
        "                predicted_probabilities = prediction.cpu().detach().numpy()\r\n",
        "            else:\r\n",
        "                predicted_probabilities = np.concatenate((predicted_probabilities, prediction.cpu().detach().numpy()))\r\n",
        "\r\n",
        "        # save predicted labels\r\n",
        "        predicted_rearranged = [0 for _ in range(len(item_array))]\r\n",
        "        for index, item in enumerate(item_array):\r\n",
        "            item = int(item)\r\n",
        "            predicted_rearranged[item] = predicted_probabilities[index]\r\n",
        "        np.save(os.path.join(path, 'test_prediction.npy'), predicted_rearranged)\r\n",
        "\r\n",
        "        return predicted_probabilities, item_array\r\n",
        "\r\n",
        "    def get_learning_rate(self, epoch):\r\n",
        "\r\n",
        "        lr = None\r\n",
        "        if epoch < self.warm_up_epochs:\r\n",
        "            lr = (epoch + 1) * self.learning_rate_step_warm\r\n",
        "        else:\r\n",
        "            if epoch < self.warm_up_epochs + self.normal_epochs:\r\n",
        "                lr = self.learning_rate\r\n",
        "            else:\r\n",
        "                epochs = epoch - self.warm_up_epochs - self.normal_epochs\r\n",
        "                lr = self.learning_rate - epochs * self.learning_rate_step_cold\r\n",
        "\r\n",
        "        print(lr, epoch)\r\n",
        "        return lr\r\n",
        "\r\n",
        "    def create_loaders(self, train, cross):\r\n",
        "        train_data = np.concatenate((self.data_dict[train[0]]['train'], self.data_dict[train[1]]['train']))\r\n",
        "        train_y = np.concatenate((self.data_dict[train[0]]['y'], self.data_dict[train[1]]['y']))\r\n",
        "\r\n",
        "        cross_data = self.data_dict[cross]['train']\r\n",
        "        cross_y = self.data_dict[cross]['y']\r\n",
        "\r\n",
        "        dataset_train = CnnDataset(train_data, train_y)\r\n",
        "        train_loader = DataLoader(dataset_train, batch_size = self.batch_size, shuffle = True)\r\n",
        "\r\n",
        "        dataset_cross = CnnDataset(cross_data, cross_y)\r\n",
        "        cross_loader = DataLoader(dataset_cross, batch_size = self.batch_size, shuffle = True)\r\n",
        "\r\n",
        "        return train_loader, cross_loader, compute_class_weight('balanced', np.unique(train_y), np.array(train_y))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzITHqEkFTvW"
      },
      "source": [
        "### TRAINING CONSTANTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpwZKCH9FVuO"
      },
      "source": [
        "# define constants influencing the learning part of the pipeline\r\n",
        "DROPOUT = 0.2\r\n",
        "\r\n",
        "NN_SECOND_LAYER_SIZE = 3\r\n",
        "\r\n",
        "NN_FIRST_LAYER_SIZE = 80\r\n",
        "\r\n",
        "NUMBER_OF_SIGNALS = 3\r\n",
        "\r\n",
        "KERNEL_SIZE = 5\r\n",
        "\r\n",
        "CNN_BLOCKS = 8\r\n",
        "\r\n",
        "NUMBER_OF_CHANNELS = 64\r\n",
        "\r\n",
        "COOLDOWN_EPOCHS = 5\r\n",
        "\r\n",
        "NORMAL_EPOCHS = 10\r\n",
        "\r\n",
        "WARM_UP_EPOCHS = 5\r\n",
        "\r\n",
        "LEARNING_RATE_DELTA = 0.00128\r\n",
        "\r\n",
        "MAX_NORM = 0.1\r\n",
        "\r\n",
        "SMOOTHING_FACTOR = 0.99\r\n",
        "\r\n",
        "BATCH_SIZE = 256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOfNsl9REzuu"
      },
      "source": [
        "### TRAINING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wzjD7mI257y"
      },
      "source": [
        "### Create suitable data format for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGY41NhbAqq8"
      },
      "source": [
        "def create_cnn_format(eeg_1, eeg_2, emg):\r\n",
        "    \r\n",
        "    data = []\r\n",
        "    for index in range(len(eeg_1)):\r\n",
        "        data.append(\r\n",
        "            [\r\n",
        "                eeg_1[index],\r\n",
        "                eeg_2[index],\r\n",
        "                emg[index]\r\n",
        "            ]\r\n",
        "        )\r\n",
        "\r\n",
        "    return np.array(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_qLe46oBMGK"
      },
      "source": [
        "# transform each subject to the suitable cnn format\r\n",
        "first_subject = create_cnn_format(first_eeg_1, first_eeg_2, first_emg)\r\n",
        "second_subject = create_cnn_format(second_eeg_1, second_eeg_2, second_emg)\r\n",
        "third_subject = create_cnn_format(third_eeg_1, third_eeg_2, third_emg)\r\n",
        "fourth_subject = create_cnn_format(fourth_eeg_1, fourth_eeg_2, fourth_emg)\r\n",
        "fifth_subject = create_cnn_format(fifth_eeg_1, fifth_eeg_2, fifth_emg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRS1iy8nBoQc"
      },
      "source": [
        "# create training dict\r\n",
        "data_dict = { }\r\n",
        "data_dict['first'] = {\r\n",
        "    'train': first_subject,\r\n",
        "    'y': first_y[:, 0]\r\n",
        "}\r\n",
        "data_dict['second'] = {\r\n",
        "    'train': second_subject,\r\n",
        "    'y': second_y[:, 0]\r\n",
        "}\r\n",
        "data_dict['third'] = {\r\n",
        "    'train': third_subject,\r\n",
        "    'y': third_y[:, 0]\r\n",
        "}\r\n",
        "data_dict['fourth'] = {\r\n",
        "    'test': fourth_subject\r\n",
        "}\r\n",
        "data_dict['fifth'] = {\r\n",
        "    'test': fifth_subject\r\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bX0xl1Y0GPCR"
      },
      "source": [
        "# force memory deallocation\r\n",
        "first_eeg_1 = None\r\n",
        "first_eeg_2 = None\r\n",
        "first_emg = None\r\n",
        "\r\n",
        "second_eeg_1 = None\r\n",
        "second_eeg_2 = None\r\n",
        "second_emg = None\r\n",
        "\r\n",
        "third_eeg_1 = None\r\n",
        "third_eeg_2 = None\r\n",
        "third_emg = None\r\n",
        "\r\n",
        "fourth_eeg_1 = None\r\n",
        "fourth_eeg_2 = None\r\n",
        "fourth_emg = None\r\n",
        "\r\n",
        "fifth_eeg_1 = None\r\n",
        "fifth_eeg_2 = None\r\n",
        "fifth_emg = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqOMqqNa3HKq"
      },
      "source": [
        "### Define and train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fKhHzdIE5zn"
      },
      "source": [
        "# define model\r\n",
        "train_engine = TrainEngine(\r\n",
        "        data_dict = data_dict,\r\n",
        "        batch_size = BATCH_SIZE,\r\n",
        "        smoothing_factor = SMOOTHING_FACTOR,\r\n",
        "        max_norm = MAX_NORM,\r\n",
        "        learning_rate_delta = LEARNING_RATE_DELTA,\r\n",
        "        warm_up_epochs = WARM_UP_EPOCHS,\r\n",
        "        normal_epochs = NORMAL_EPOCHS,\r\n",
        "        cooldown_epochs = COOLDOWN_EPOCHS,\r\n",
        "        number_of_channels = NUMBER_OF_CHANNELS,\r\n",
        "        cnn_blocks = CNN_BLOCKS,\r\n",
        "        example_length = first_subject.shape[-1],\r\n",
        "        kernel_size = KERNEL_SIZE,\r\n",
        "        number_of_signals = NUMBER_OF_SIGNALS,\r\n",
        "        nn_first_layer_size = NN_FIRST_LAYER_SIZE,\r\n",
        "        nn_second_layer_size = NN_SECOND_LAYER_SIZE,\r\n",
        "        dropout = DROPOUT,\r\n",
        "        use_gpu = True\r\n",
        "    )\r\n",
        "\r\n",
        "# perform LOOCV training\r\n",
        "real, predicted = train_engine.train(base_path)\r\n",
        "\r\n",
        "# perform full training, without \r\n",
        "predicted_probabilities, item_array = train_engine.full_train(base_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8V5ueGLNT2Rt"
      },
      "source": [
        "# dataset_train = CnnDataset(data_dict['third']['train'], data_dict['third']['y'])\r\n",
        "# train_loader = DataLoader(dataset_train, batch_size = 256, shuffle = True)\r\n",
        "\r\n",
        "# model = train_engine.model\r\n",
        "# model.to(torch.device(\"cuda:0\"))\r\n",
        "\r\n",
        "# predicted_array = np.array([])\r\n",
        "# real_array = np.array([])\r\n",
        "# item_array = np.array([])\r\n",
        "\r\n",
        "# for batch, real, item in train_loader:\r\n",
        "#   batch = batch.to(torch.device(\"cuda:0\"))\r\n",
        "\r\n",
        "#   # change to float type\r\n",
        "#   batch = batch.float()\r\n",
        "\r\n",
        "#   # get model prediction\r\n",
        "#   prediction = model(batch)\r\n",
        "\r\n",
        "#   predicted_labels = torch.argmax(prediction, dim = 1) + 1\r\n",
        "#   predicted_labels = predicted_labels.cpu().detach().numpy()\r\n",
        "#   real = real.cpu().detach().numpy()\r\n",
        "#   item = item.cpu().detach().numpy()\r\n",
        "#   predicted_array = np.concatenate((predicted_array, predicted_labels))\r\n",
        "#   real_array = np.concatenate((real_array, real))\r\n",
        "#   item_array = np.concatenate((item_array, item))\r\n",
        "\r\n",
        "# print(balanced_accuracy_score(real_array, predicted_array))\r\n",
        "# print(item_array)\r\n",
        "\r\n",
        "# predicted_rearranged = [0 for _ in range(21600 - 4)]\r\n",
        "# for index, item in enumerate(item_array):\r\n",
        "#   item = int(item)\r\n",
        "#   predicted_rearranged[item] = predicted_array[index]\r\n",
        "\r\n",
        "# real = third_y[:,0]\r\n",
        "# balanced_accuracy_score(real, predicted_rearranged)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DWR53bY0h8F"
      },
      "source": [
        "# prediction = np.argmax(predicted_probabilities, axis = 1)\r\n",
        "\r\n",
        "# predicted_rearranged = [0 for _ in range(len(item_array))]\r\n",
        "# for index, item in enumerate(item_array):\r\n",
        "#   item = int(item)\r\n",
        "#   predicted_rearranged[item] = prediction[index]\r\n",
        "\r\n",
        "# prediction = predicted_rearranged\r\n",
        "# prediction = np.array(prediction)\r\n",
        "\r\n",
        "# middle = prediction.shape[0] // 2\r\n",
        "# fourth_prediction = prediction[:middle]\r\n",
        "# fifth_prediction = prediction[middle:]\r\n",
        "\r\n",
        "# final_prediction = np.concatenate((\r\n",
        "#     np.array([fourth_prediction[0], fourth_prediction[0]]),\r\n",
        "#     fourth_prediction,\r\n",
        "#     np.array([fourth_prediction[-1], fourth_prediction[-1]]),\r\n",
        "#     np.array([fifth_prediction[0], fifth_prediction[0]]),\r\n",
        "#     fifth_prediction,\r\n",
        "#     np.array([fifth_prediction[-1], fifth_prediction[-1]])\r\n",
        "# ))\r\n",
        "\r\n",
        "# # create submission\r\n",
        "# submission = []\r\n",
        "# for index in range(len(final_prediction)):\r\n",
        "#   submission.append(\r\n",
        "#       [\r\n",
        "#         index,\r\n",
        "#         final_prediction[index] + 1\r\n",
        "#       ]\r\n",
        "#   )\r\n",
        "\r\n",
        "# export = pd.DataFrame(data = np.array(submission), columns = ['Id','y'])\r\n",
        "# export.to_csv(os.path.join(base_path, f'submission_initial.csv'), encoding = 'utf-8', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}